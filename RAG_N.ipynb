{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Consolidate all installations needed for Task 2 (Embeddings and FAISS)\n",
        "!pip install sentence-transformers langchain-huggingface faiss-cpu langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktp-U1PCHDIv",
        "outputId": "7c260f2c-ffd3-465f-beb0-53daae4e3018"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.12/dist-packages (0.0.3)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.0)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.0.38)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: langchain-core<0.3,>=0.1.52 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.1.52)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.3,>=0.1.52->langchain-huggingface) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.3,>=0.1.52->langchain-huggingface) (0.1.147)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.3,>=0.1.52->langchain-huggingface) (2.12.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.3,>=0.1.52->langchain-huggingface) (8.5.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.52->langchain-huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.52->langchain-huggingface) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.52->langchain-huggingface) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.52->langchain-huggingface) (1.0.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.52->langchain-huggingface) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.52->langchain-huggingface) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.52->langchain-huggingface) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.52->langchain-huggingface) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.52->langchain-huggingface) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain-huggingface) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain-huggingface) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.44)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.52->langchain-huggingface) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete clean installation with compatible versions\n",
        "!pip uninstall -y langchain langchain-community langchain-core langchain-huggingface\n",
        "!pip install --upgrade pip\n",
        "!pip install langchain==0.1.20 langchain-community==0.0.38 langchain-core==0.1.52 langchain-huggingface transformers accelerate bitsandbytes sentence-transformers faiss-cpu\n",
        "\n",
        "# RESTART RUNTIME after installation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4h61pQK4HIln",
        "outputId": "83c6d828-34d2-47fd-d351-169ad35086ff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: langchain 0.1.20\n",
            "Uninstalling langchain-0.1.20:\n",
            "  Successfully uninstalled langchain-0.1.20\n",
            "Found existing installation: langchain-community 0.0.38\n",
            "Uninstalling langchain-community-0.0.38:\n",
            "  Successfully uninstalled langchain-community-0.0.38\n",
            "Found existing installation: langchain-core 0.1.52\n",
            "Uninstalling langchain-core-0.1.52:\n",
            "  Successfully uninstalled langchain-core-0.1.52\n",
            "Found existing installation: langchain-huggingface 0.0.3\n",
            "Uninstalling langchain-huggingface-0.0.3:\n",
            "  Successfully uninstalled langchain-huggingface-0.0.3\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.3)\n",
            "Collecting langchain==0.1.20\n",
            "  Using cached langchain-0.1.20-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting langchain-community==0.0.38\n",
            "  Using cached langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting langchain-core==0.1.52\n",
            "  Using cached langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langchain-huggingface\n",
            "  Using cached langchain_huggingface-1.1.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.1.20) (6.0.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain==0.1.20) (2.0.44)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.1.20) (3.13.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.12/dist-packages (from langchain==0.1.20) (0.6.7)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from langchain==0.1.20) (0.0.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain==0.1.20) (0.1.147)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.12/dist-packages (from langchain==0.1.20) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.12/dist-packages (from langchain==0.1.20) (2.12.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain==0.1.20) (2.32.5)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain==0.1.20) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core==0.1.52) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core==0.1.52) (23.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.20) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.20) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.20) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.20) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.20) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.20) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.20) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.20) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.20) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core==0.1.52) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.20) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.20) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.20) (1.0.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.20) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.20) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.20) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.20) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.20) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain==0.1.20) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain==0.1.20) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain==0.1.20) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain==0.1.20) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.1.20) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.1.20) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.20) (3.2.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.20) (1.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.33.4 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.36.0)\n",
            "INFO: pip is looking at multiple versions of langchain-huggingface to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached langchain_huggingface-1.0.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "  Using cached langchain_huggingface-1.0.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "  Using cached langchain_huggingface-0.3.1-py3-none-any.whl.metadata (996 bytes)\n",
            "  Using cached langchain_huggingface-0.3.0-py3-none-any.whl.metadata (996 bytes)\n",
            "  Using cached langchain_huggingface-0.2.0-py3-none-any.whl.metadata (941 bytes)\n",
            "  Using cached langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Using cached langchain_huggingface-0.1.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "INFO: pip is still looking at multiple versions of langchain-huggingface to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached langchain_huggingface-0.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Using cached langchain_huggingface-0.0.3-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.22.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (1.2.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.20) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Using cached langchain-0.1.20-py3-none-any.whl (1.0 MB)\n",
            "Using cached langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n",
            "Using cached langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
            "Using cached langchain_huggingface-0.0.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: langchain-core, langchain-community, langchain-huggingface, langchain\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [langchain]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-classic 1.0.0 requires langchain-core<2.0.0,>=1.0.0, but you have langchain-core 0.1.52 which is incompatible.\n",
            "langchain-classic 1.0.0 requires langchain-text-splitters<2.0.0,>=1.0.0, but you have langchain-text-splitters 0.0.2 which is incompatible.\n",
            "langgraph-prebuilt 1.0.5 requires langchain-core>=1.0.0, but you have langchain-core 0.1.52 which is incompatible.\n",
            "langgraph-checkpoint 3.0.1 requires langchain-core>=0.2.38, but you have langchain-core 0.1.52 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-0.1.20 langchain-community-0.0.38 langchain-core-0.1.52 langchain-huggingface-0.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete clean installation with compatible versions\n",
        "!pip uninstall -y langchain langchain-community langchain-core langchain-huggingface\n",
        "!pip install --upgrade pip\n",
        "!pip install langchain==0.1.20 langchain-community==0.0.38 langchain-core==0.1.52 langchain-huggingface transformers accelerate bitsandbytes sentence-transformers faiss-cpu\n",
        "\n",
        "# RESTART RUNTIME after installation"
      ],
      "metadata": {
        "id": "1H33oq6QHbNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Exploration\n"
      ],
      "metadata": {
        "id": "7uJOOZoFrWCW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J047twc-Om8r",
        "outputId": "d54dfa34-0196-4ad8-d2a2-3fabcba7f6e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using ZIP file: /content/mimic-iv-ext-direct-1.0.0.zip\n",
            "Starting extraction of mimic-iv-ext-direct-1.0.0.zip...\n",
            "Main ZIP extraction successful.\n",
            "\n",
            "--- Contents inside the inner folder (./MIMIC_RAG_Project_Data/mimic-iv-ext-direct-1.0.0) ---\n",
            "FILE: .DS_Store\n",
            "COMPRESSED FILE: diagnostic_kg.rar\n",
            "COMPRESSED FILE: samples.rar\n",
            "FOLDER: Finished\n",
            "FILE: LICENSE.txt\n",
            "FILE: README.md\n",
            "FILE: SHA256SUMS.txt\n",
            "\n",
            "Confirmed: All required components are present.\n",
            "\n",
            "Extracting samples.rar...\n",
            "Extraction complete: samples.rar\n",
            "Items extracted to ./DiReCT_Notes_Sample/ (first 5): ['Finished']\n",
            "\n",
            "Extracting diagnostic_kg.rar...\n",
            "Extraction complete: diagnostic_kg.rar\n",
            "Items extracted to ./DiReCT_Knowledge_Graph/ (first 5): ['Diagnosis_flowchart']\n",
            "\n",
            "All compressed files extracted successfully. Ready for data loading.\n"
          ]
        }
      ],
      "source": [
        "# ==========================================================\n",
        "#   FULL EXTRACTION + VALIDATION PIPELINE (ONE CELL)\n",
        "# ==========================================================\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import rarfile\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# STEP 1 — Extract Main ZIP File\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "MAIN_ZIP_FILE_PATH = '/content/mimic-iv-ext-direct-1.0.0.zip'\n",
        "EXTRACT_ROOT_DIR = './MIMIC_RAG_Project_Data/'\n",
        "os.makedirs(EXTRACT_ROOT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Using ZIP file: {MAIN_ZIP_FILE_PATH}\")\n",
        "print(f\"Starting extraction of {MAIN_ZIP_FILE_PATH.split('/')[-1]}...\")\n",
        "\n",
        "try:\n",
        "    with zipfile.ZipFile(MAIN_ZIP_FILE_PATH, 'r') as zip_ref:\n",
        "        zip_ref.extractall(EXTRACT_ROOT_DIR)\n",
        "    print(\"Main ZIP extraction successful.\\n\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: File not found at {MAIN_ZIP_FILE_PATH}\")\n",
        "except Exception as e:\n",
        "    print(f\"Unexpected error during ZIP extraction: {e}\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# STEP 2 — Check Inner Folder Contents\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "INNER_FOLDER_PATH = './MIMIC_RAG_Project_Data/mimic-iv-ext-direct-1.0.0'\n",
        "print(f\"--- Contents inside the inner folder ({INNER_FOLDER_PATH}) ---\")\n",
        "\n",
        "try:\n",
        "    inner_contents = os.listdir(INNER_FOLDER_PATH)\n",
        "\n",
        "    for item in inner_contents:\n",
        "        full_path = os.path.join(INNER_FOLDER_PATH, item)\n",
        "        if os.path.isdir(full_path):\n",
        "            print(f\"FOLDER: {item}\")\n",
        "        elif os.path.isfile(full_path):\n",
        "            if item.endswith('.rar') or item.endswith('.zip'):\n",
        "                print(f\"COMPRESSED FILE: {item}\")\n",
        "            else:\n",
        "                print(f\"FILE: {item}\")\n",
        "        else:\n",
        "            print(f\"OTHER: {item}\")\n",
        "\n",
        "    key_files_found = {\n",
        "        'Finished_Folder': 'Finished' in inner_contents,\n",
        "        'samples.rar_file': 'samples.rar' in inner_contents,\n",
        "        'diagnostic_kg.rar_file': 'diagnostic_kg.rar' in inner_contents\n",
        "    }\n",
        "\n",
        "    if all(key_files_found.values()):\n",
        "        print(\"\\nConfirmed: All required components are present.\")\n",
        "    else:\n",
        "        print(\"\\nWarning: Missing one or more required files.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: Inner folder not found at {INNER_FOLDER_PATH}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error while checking inner folder: {e}\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# STEP 3 — Extract RAR Files (Samples + Knowledge Graph)\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "SAMPLES_RAR_PATH = os.path.join(INNER_FOLDER_PATH, 'samples.rar')\n",
        "KG_RAR_PATH = os.path.join(INNER_FOLDER_PATH, 'diagnostic_kg.rar')\n",
        "\n",
        "SAMPLES_EXTRACT_DIR = './DiReCT_Notes_Sample/'\n",
        "KG_EXTRACT_DIR = './DiReCT_Knowledge_Graph/'\n",
        "\n",
        "os.makedirs(SAMPLES_EXTRACT_DIR, exist_ok=True)\n",
        "os.makedirs(KG_EXTRACT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "def extract_rar(rar_path, extract_dir):\n",
        "    file_name = os.path.basename(rar_path)\n",
        "    print(f\"\\nExtracting {file_name}...\")\n",
        "\n",
        "    try:\n",
        "        with rarfile.RarFile(rar_path) as rf:\n",
        "            rf.extractall(extract_dir)\n",
        "        print(f\"Extraction complete: {file_name}\")\n",
        "\n",
        "        extracted_contents = os.listdir(extract_dir)\n",
        "        print(f\"Items extracted to {extract_dir} (first 5): {extracted_contents[:5]}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: RAR file not found at {rar_path}\")\n",
        "    except rarfile.RarCannotExec:\n",
        "        print(\"ERROR: 'unrar' tool missing. Install using: !apt-get install unrar\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during extraction of {file_name}: {e}\")\n",
        "\n",
        "\n",
        "extract_rar(SAMPLES_RAR_PATH, SAMPLES_EXTRACT_DIR)\n",
        "extract_rar(KG_RAR_PATH, KG_EXTRACT_DIR)\n",
        "\n",
        "print(\"\\nAll compressed files extracted successfully. Ready for data loading.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the command-line utility for handling .rar files\n",
        "!apt-get install unrar\n",
        "\n",
        "# Install the Python wrapper library\n",
        "!pip install rarfile"
      ],
      "metadata": {
        "id": "9wgV236gO466",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5faa67d3-d64b-4d4a-e343-4e8b5e6b9b3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "unrar is already the newest version (1:6.1.5-1ubuntu0.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
            "Requirement already satisfied: rarfile in /usr/local/lib/python3.12/dist-packages (4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and Parsing Clinical Notes"
      ],
      "metadata": {
        "id": "IyEJA9oRqqcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Correct shared path\n",
        "CORRECTED_ROOT_DIR = './DiReCT_Notes_Sample/Finished'\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "#  PART 1 — LOADER VERSION 1\n",
        "#  (Subjective / Objective / Assessment / Plan)\n",
        "# ===============================================================\n",
        "\n",
        "def load_clinical_notes_v1(root_dir):\n",
        "    \"\"\"\n",
        "    Loads JSON clinical notes and extracts SOAP-style narrative fields.\n",
        "    \"\"\"\n",
        "    all_notes = []\n",
        "\n",
        "    for root, _, files in os.walk(root_dir):\n",
        "        for file in files:\n",
        "            if file.endswith('.json'):\n",
        "                file_path = os.path.join(root, file)\n",
        "                try:\n",
        "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                        note_data = json.load(f)\n",
        "\n",
        "                    subjective = note_data.get('Subjective', '')\n",
        "                    objective = note_data.get('Objective', '')\n",
        "                    assessment = note_data.get('Assessment', '')\n",
        "                    plan = note_data.get('Plan', '')\n",
        "\n",
        "                    full_text = (\n",
        "                        f\"Subjective: {subjective}\\n\"\n",
        "                        f\"Objective: {objective}\\n\"\n",
        "                        f\"Assessment: {assessment}\\n\"\n",
        "                        f\"Plan: {plan}\"\n",
        "                    )\n",
        "\n",
        "                    all_notes.append({\n",
        "                        'note_id': note_data.get('ID', os.path.basename(file)),\n",
        "                        'full_text': full_text,\n",
        "                        'diagnosis': note_data.get('Diagnosis', 'N/A'),\n",
        "                        'abstract': note_data.get('Abstract', ''),\n",
        "                        'reasoning_steps': note_data.get('Reasoning_steps', [])\n",
        "                    })\n",
        "\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "    return pd.DataFrame(all_notes)\n",
        "\n",
        "\n",
        "print(f\"Loading Version 1 Notes from: {CORRECTED_ROOT_DIR}\")\n",
        "df_notes_v1 = load_clinical_notes_v1(CORRECTED_ROOT_DIR)\n",
        "print(\"Version 1 Loaded:\", len(df_notes_v1), \"notes\")\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "#  PART 2 — JSON KEY INSPECTION\n",
        "#  (Finds keys from one sample file)\n",
        "# ===============================================================\n",
        "\n",
        "def find_note_body_key(root_dir):\n",
        "    \"\"\"\n",
        "    Prints all top-level keys in the first JSON file found.\n",
        "    \"\"\"\n",
        "    for root, _, files in os.walk(root_dir):\n",
        "        for file in files:\n",
        "            if file.endswith('.json'):\n",
        "                fpath = os.path.join(root, file)\n",
        "                try:\n",
        "                    with open(fpath, 'r', encoding='utf-8') as f:\n",
        "                        sample = json.load(f)\n",
        "\n",
        "                    print(f\"\\n--- Keys in sample file: {file} ---\")\n",
        "                    for key, value in sample.items():\n",
        "                        snippet = str(value)[:150].replace(\"\\n\", \" \")\n",
        "                        print(f\"Key: {key} | Type: {type(value).__name__} | Snippet: {snippet}...\")\n",
        "                    return\n",
        "\n",
        "                except Exception:\n",
        "                    continue\n",
        "    print(\"No JSON files found.\")\n",
        "\n",
        "\n",
        "print(\"\\nInspecting JSON Keys...\")\n",
        "find_note_body_key(CORRECTED_ROOT_DIR)\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "#  PART 3 — LOADER VERSION 2\n",
        "#  (input1–input6 structured format + diagnosis from key)\n",
        "# ===============================================================\n",
        "\n",
        "def load_clinical_notes_v2(root_dir):\n",
        "    \"\"\"\n",
        "    Loads JSON notes using 'input1' to 'input6'\n",
        "    and extracts diagnosis from complex keys.\n",
        "    \"\"\"\n",
        "    all_notes = []\n",
        "\n",
        "    for root, _, files in os.walk(root_dir):\n",
        "        for file in files:\n",
        "            if file.endswith('.json'):\n",
        "                fpath = os.path.join(root, file)\n",
        "                try:\n",
        "                    with open(fpath, 'r', encoding='utf-8') as f:\n",
        "                        note_data = json.load(f)\n",
        "\n",
        "                    complaint = note_data.get('input1', '')\n",
        "                    hpi = note_data.get('input2', '')\n",
        "                    pmh = note_data.get('input3', '')\n",
        "                    fh = note_data.get('input4', '')\n",
        "                    pe = note_data.get('input5', '')\n",
        "                    labs = note_data.get('input6', '')\n",
        "\n",
        "                    full_text = (\n",
        "                        f\"CHIEF COMPLAINT: {complaint}\\n\"\n",
        "                        f\"HISTORY OF PRESENT ILLNESS: {hpi}\\n\"\n",
        "                        f\"PAST MEDICAL HISTORY: {pmh}\\n\"\n",
        "                        f\"FAMILY HISTORY: {fh}\\n\"\n",
        "                        f\"PHYSICAL EXAMINATION: {pe}\\n\"\n",
        "                        f\"LABS/IMAGING: {labs}\"\n",
        "                    )\n",
        "\n",
        "                    # Extract diagnosis from the first non-input key\n",
        "                    diag_key = [k for k in note_data if not k.startswith(\"input\")][0]\n",
        "                    diagnosis = diag_key.split(\"$\")[0]\n",
        "\n",
        "                    all_notes.append({\n",
        "                        'note_id': os.path.basename(fpath),\n",
        "                        'full_text': full_text,\n",
        "                        'diagnosis': diagnosis\n",
        "                    })\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {fpath}. Skipping. Error: {e}\")\n",
        "                    continue\n",
        "\n",
        "    return pd.DataFrame(all_notes)\n",
        "\n",
        "\n",
        "print(\"\\nLoading Version 2 Notes...\")\n",
        "df_notes_v2 = load_clinical_notes_v2(CORRECTED_ROOT_DIR)\n",
        "print(\"Version 2 Loaded:\", len(df_notes_v2), \"notes\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hihxpc45qO8d",
        "outputId": "d6916d86-b899-44e2-e198-a25e281ed40f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Version 1 Notes from: ./DiReCT_Notes_Sample/Finished\n",
            "Version 1 Loaded: 511 notes\n",
            "\n",
            "Inspecting JSON Keys...\n",
            "\n",
            "--- Keys in sample file: 18427803-DS-5.json ---\n",
            "Key: Migraine With Aura$Intermedia_3 | Type: dict | Snippet: {'Difficulty expressing language may be associated with migraine, especially when migraine is accompanied by neurological symptoms$Cause_1': {'Difficu...\n",
            "Key: input1 | Type: str | Snippet: Difficulty producing speech  ...\n",
            "Key: input2 | Type: str | Snippet: Patient woke up at about 0900 AM. Patient felt that she had a migraine. Patient took acetaminophen and went back to bed. Patient woke back up one hour...\n",
            "Key: input3 | Type: str | Snippet: Hypothyroidism  HLD  History of breast CA  Left paramedian pontine stroke  ...\n",
            "Key: input4 | Type: str | Snippet: Mother had heart disease in elderly age. Paternal grandmother had stroke in ___.  ...\n",
            "Key: input5 | Type: str | Snippet: PHYSICAL EXAMINATION:  Presentation vitals:  Temperature: 98.5  Heart rate: 113  Blood pressure: 133/85  Respiratory rate: 15  Oxygen saturation: 96% ...\n",
            "Key: input6 | Type: str | Snippet: ___ 01:09PM BLOOD WBC-6.1 RBC-4.34 Hgb-12.1 Hct-37.6 MCV-87  MCH-27.9 MCHC-32.2 RDW-13.2 RDWSD-42.0 Plt ___ ___ 01:09PM BLOOD Neuts-58.6 ___ Monos-10....\n",
            "\n",
            "Loading Version 2 Notes...\n",
            "Version 2 Loaded: 511 notes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Corrected Data Loading and Parsing"
      ],
      "metadata": {
        "id": "j0YkKfVFrLKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# KNOWLEDGE GRAPH DATA EXPLORATION (CLEAN VERSION, NO EMOJIS, NO REDUNDANCY)\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "KG_ROOT_DIR = './DiReCT_Knowledge_Graph'\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"EXPLORING KNOWLEDGE GRAPH STRUCTURE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# List directory structure\n",
        "print(f\"\\nExploring directory: {KG_ROOT_DIR}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for root, dirs, files in os.walk(KG_ROOT_DIR):\n",
        "    level = root.replace(KG_ROOT_DIR, '').count(os.sep)\n",
        "    indent = ' ' * 2 * level\n",
        "    print(f\"{indent}{os.path.basename(root)}/\")\n",
        "\n",
        "    sub_indent = ' ' * 2 * (level + 1)\n",
        "    for file in files:\n",
        "        file_size = os.path.getsize(os.path.join(root, file))\n",
        "        print(f\"{sub_indent}{file} ({file_size} bytes)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"INSPECTING SAMPLE KNOWLEDGE GRAPH FILE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Find the first JSON file\n",
        "sample_kg_file = None\n",
        "for root, _, files in os.walk(KG_ROOT_DIR):\n",
        "    for file in files:\n",
        "        if file.endswith('.json'):\n",
        "            sample_kg_file = os.path.join(root, file)\n",
        "            break\n",
        "    if sample_kg_file:\n",
        "        break\n",
        "\n",
        "if sample_kg_file:\n",
        "    print(f\"\\nSample file: {os.path.basename(sample_kg_file)}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    try:\n",
        "        with open(sample_kg_file, 'r', encoding='utf-8') as f:\n",
        "            sample_kg_data = json.load(f)\n",
        "\n",
        "        print(\"File loaded successfully\\n\")\n",
        "        print(\"Top-level structure:\\n\")\n",
        "\n",
        "        def inspect_structure(data, depth=0, max_depth=3):\n",
        "            \"\"\"Recursively inspect JSON structure\"\"\"\n",
        "            indent = \"  \" * depth\n",
        "            if depth > max_depth:\n",
        "                return\n",
        "\n",
        "            if isinstance(data, dict):\n",
        "                for i, (key, value) in enumerate(data.items()):\n",
        "                    if i >= 5:\n",
        "                        print(f\"{indent}... ({len(data)-5} more keys)\")\n",
        "                        break\n",
        "\n",
        "                    clean_key = key.split('$')[0] if '$' in key else key\n",
        "                    if isinstance(value, dict):\n",
        "                        print(f\"{indent}{clean_key} (dict with {len(value)} keys)\")\n",
        "                        inspect_structure(value, depth + 1)\n",
        "                    elif isinstance(value, list):\n",
        "                        print(f\"{indent}{clean_key} (list with {len(value)} items)\")\n",
        "                        if value:\n",
        "                            inspect_structure(value[0], depth + 1)\n",
        "                    elif isinstance(value, str):\n",
        "                        print(f\"{indent}{clean_key}: {value[:80].replace(chr(10), ' ')}...\")\n",
        "                    else:\n",
        "                        print(f\"{indent}{clean_key}: {value}\")\n",
        "\n",
        "            elif isinstance(data, list) and data:\n",
        "                print(f\"{indent}List with {len(data)} items\")\n",
        "                inspect_structure(data[0], depth + 1)\n",
        "\n",
        "        inspect_structure(sample_kg_data)\n",
        "\n",
        "        print(\"\\nRaw JSON preview (first 500 chars):\")\n",
        "        print(\"-\" * 70)\n",
        "        print(json.dumps(sample_kg_data, indent=2)[:500] + \"...\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading sample file: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"No JSON files found in Knowledge Graph directory\")\n",
        "\n",
        "# =============================================================================\n",
        "# LOAD ALL KNOWLEDGE GRAPH FILES\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"LOADING ALL KNOWLEDGE GRAPH FILES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "def load_knowledge_graph_files(root_dir):\n",
        "    \"\"\"Load and flatten all knowledge graph JSON files.\"\"\"\n",
        "    kg_data = []\n",
        "\n",
        "    for root, _, files in os.walk(root_dir):\n",
        "        for file in files:\n",
        "            if file.endswith('.json'):\n",
        "                file_path = os.path.join(root, file)\n",
        "                try:\n",
        "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                        data = json.load(f)\n",
        "\n",
        "                    diagnosis_name = file.replace('.json', '')\n",
        "                    kg_text = extract_kg_text(data)\n",
        "\n",
        "                    kg_data.append({\n",
        "                        'source_file': file,\n",
        "                        'diagnosis': diagnosis_name,\n",
        "                        'raw_data': data,\n",
        "                        'text_content': kg_text,\n",
        "                        'num_keys': count_keys(data)\n",
        "                    })\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading {file}: {e}\")\n",
        "                    continue\n",
        "\n",
        "    return pd.DataFrame(kg_data)\n",
        "\n",
        "def count_keys(data):\n",
        "    \"\"\"Count keys in nested dict/list.\"\"\"\n",
        "    if isinstance(data, dict):\n",
        "        return len(data) + sum(count_keys(v) for v in data.values())\n",
        "    if isinstance(data, list):\n",
        "        return sum(count_keys(i) for i in data)\n",
        "    return 0\n",
        "\n",
        "def extract_kg_text(data):\n",
        "    \"\"\"Flatten nested knowledge graph into readable text.\"\"\"\n",
        "    parts = []\n",
        "\n",
        "    def traverse(obj, depth=0):\n",
        "        indent = \"  \" * depth\n",
        "        if isinstance(obj, dict):\n",
        "            for key, value in obj.items():\n",
        "                clean_key = key.split('$')[0]\n",
        "                parts.append(f\"{indent}• {clean_key}\")\n",
        "                traverse(value, depth + 1)\n",
        "\n",
        "        elif isinstance(obj, list):\n",
        "            for item in obj:\n",
        "                traverse(item, depth)\n",
        "\n",
        "        elif isinstance(obj, str) and obj.strip():\n",
        "            parts.append(f\"{indent}- {obj.strip()}\")\n",
        "\n",
        "    traverse(data)\n",
        "    return \"\\n\".join(parts)\n",
        "\n",
        "print(f\"\\nLoading knowledge graph files from: {KG_ROOT_DIR}\")\n",
        "df_kg = load_knowledge_graph_files(KG_ROOT_DIR)\n",
        "\n",
        "# =============================================================================\n",
        "# KNOWLEDGE GRAPH SUMMARY\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"KNOWLEDGE GRAPH SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if not df_kg.empty:\n",
        "    print(f\"\\nTotal knowledge graph files loaded: {len(df_kg)}\")\n",
        "\n",
        "    print(\"\\nLoaded Diagnoses:\")\n",
        "    for idx, row in df_kg.iterrows():\n",
        "        print(f\"{idx+1}. {row['diagnosis']} ({row['num_keys']} keys, {len(row['text_content'])} chars)\")\n",
        "\n",
        "    print(f\"\\nTotal files: {len(df_kg)}\")\n",
        "    print(f\"Total keys: {df_kg['num_keys'].sum()}\")\n",
        "    print(f\"Avg keys per file: {df_kg['num_keys'].mean():.1f}\")\n",
        "    print(f\"Total text length: {df_kg['text_content'].str.len().sum():,}\")\n",
        "\n",
        "    print(\"\\nSAMPLE KNOWLEDGE GRAPH ENTRY\")\n",
        "    sample = df_kg.iloc[0]\n",
        "    print(f\"\\nDiagnosis: {sample['diagnosis']}\")\n",
        "    print(f\"Source: {sample['source_file']}\")\n",
        "    print(f\"Number of keys: {sample['num_keys']}\")\n",
        "    print(\"\\nContent Preview:\")\n",
        "    print(\"-\" * 70)\n",
        "    print(sample['text_content'][:800])\n",
        "    print(\"\\n... (content continues)\")\n",
        "else:\n",
        "    print(\"No knowledge graph files were loaded\")\n",
        "\n",
        "# =============================================================================\n",
        "# DEPENDENCY: df_notes MUST EXIST (FROM EARLIER CLINICAL NOTE LOADER)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"DATASET COMPARISON SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if 'df_notes' in globals() and not df_notes.empty and not df_kg.empty:\n",
        "    print(f\"\\nClinical Notes:    {len(df_notes)} documents\")\n",
        "    print(f\"Knowledge Graph:   {len(df_kg)} documents\")\n",
        "\n",
        "    kg_diagnoses = set(df_kg['diagnosis'])\n",
        "    note_diagnoses = set(df_notes['diagnosis'])\n",
        "\n",
        "    overlap = kg_diagnoses & note_diagnoses\n",
        "\n",
        "    print(f\"\\nUnique diagnoses in KG:    {len(kg_diagnoses)}\")\n",
        "    print(f\"Unique diagnoses in Notes: {len(note_diagnoses)}\")\n",
        "    print(f\"Overlapping diagnoses:     {len(overlap)}\")\n",
        "\n",
        "    if overlap:\n",
        "        print(\"\\nCommon Diagnoses (sample):\")\n",
        "        for diag in list(overlap)[:5]:\n",
        "            print(f\"{diag}\")\n",
        "\n",
        "    print(\"\\nDATA TYPE COMPARISON\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    sample_note = df_notes.iloc[0]\n",
        "    print(\"\\nCLINICAL NOTE SAMPLE:\")\n",
        "    print(f\"Note ID: {sample_note['note_id']}\")\n",
        "    print(f\"Diagnosis: {sample_note['diagnosis']}\")\n",
        "    print(sample_note['full_text'][:300], \"...\")\n",
        "\n",
        "    sample_kg = df_kg.iloc[0]\n",
        "    print(\"\\nKNOWLEDGE GRAPH SAMPLE:\")\n",
        "    print(f\"Diagnosis: {sample_kg['diagnosis']}\")\n",
        "    print(sample_kg['text_content'][:300], \"...\")\n",
        "\n",
        "    print(\"\\nKEY DIFFERENCES\")\n",
        "    print(\"- Clinical Notes: patient-specific, narrative, single case\")\n",
        "    print(\"- Knowledge Graph: generalized diagnostic rules, structured\")\n",
        "else:\n",
        "    print(\"df_notes missing or empty. Cannot compare.\")\n",
        "\n",
        "print(\"\\nDATA LOADING COMPLETE\")\n",
        "print(\"=\" * 70)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFTOkTbMxNti",
        "outputId": "da13ad27-415d-497e-9ef9-c5afa9100852"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "EXPLORING KNOWLEDGE GRAPH STRUCTURE\n",
            "======================================================================\n",
            "\n",
            "Exploring directory: ./DiReCT_Knowledge_Graph\n",
            "----------------------------------------------------------------------\n",
            "DiReCT_Knowledge_Graph/\n",
            "  Diagnosis_flowchart/\n",
            "    Upper Gastrointestinal Bleeding.json (1067 bytes)\n",
            "    Acute Coronary Syndrome.json (1020 bytes)\n",
            "    Heart Failure.json (1726 bytes)\n",
            "    Multiple Sclerosis.json (3675 bytes)\n",
            "    Stroke.json (2833 bytes)\n",
            "    Adrenal Insufficiency.json (2583 bytes)\n",
            "    Epilepsy.json (1992 bytes)\n",
            "    Hyperlipidemia.json (1059 bytes)\n",
            "    Cardiomyopathy.json (2493 bytes)\n",
            "    Hypertension.json (658 bytes)\n",
            "    Tuberculosis.json (2633 bytes)\n",
            "    Aortic Dissection.json (824 bytes)\n",
            "    Peptic Ulcer Disease.json (1560 bytes)\n",
            "    COPD.json (1038 bytes)\n",
            "    Pneumonia.json (1961 bytes)\n",
            "    Migraine.json (1512 bytes)\n",
            "    Pulmonary Embolism.json (3324 bytes)\n",
            "    Gastro-oesophageal Reflux Disease.json (1906 bytes)\n",
            "    Diabetes.json (3107 bytes)\n",
            "    Alzheimer.json (2522 bytes)\n",
            "    Asthma.json (2502 bytes)\n",
            "    Thyroid Disease.json (3740 bytes)\n",
            "    Atrial Fibrillation.json (1037 bytes)\n",
            "    Pituitary Disease.json (2088 bytes)\n",
            "\n",
            "======================================================================\n",
            "INSPECTING SAMPLE KNOWLEDGE GRAPH FILE\n",
            "======================================================================\n",
            "\n",
            "Sample file: Upper Gastrointestinal Bleeding.json\n",
            "----------------------------------------------------------------------\n",
            "File loaded successfully\n",
            "\n",
            "Top-level structure:\n",
            "\n",
            "diagnostic (dict with 1 keys)\n",
            "  Suspected Upper Gastrointestinal Bleeding (dict with 1 keys)\n",
            "    Upper Gastrointestinal Bleeding (list with 0 items)\n",
            "knowledge (dict with 2 keys)\n",
            "  Suspected Upper Gastrointestinal Bleeding (dict with 2 keys)\n",
            "    Risk Factors: peptic ulcers; prolonged use of nonsteroidal anti-inflammatory drugs (NSAIDs); H...\n",
            "    Symptoms: hematemesis (vomiting of red blood or coffee-grounds material); melena (black, t...\n",
            "  Upper Gastrointestinal Bleeding: Bleeding outside the digestive tract was excluded: Melena caused by eating and b...\n",
            "\n",
            "Raw JSON preview (first 500 chars):\n",
            "----------------------------------------------------------------------\n",
            "{\n",
            "  \"diagnostic\": {\n",
            "    \"Suspected Upper Gastrointestinal Bleeding\": {\n",
            "      \"Upper Gastrointestinal Bleeding\": []\n",
            "    }\n",
            "  },\n",
            "  \"knowledge\": {\n",
            "    \"Suspected Upper Gastrointestinal Bleeding\": {\n",
            "      \"Risk Factors\": \"peptic ulcers; prolonged use of nonsteroidal anti-inflammatory drugs (NSAIDs); Helicobacter pylori infection; esophageal varices; alcohol abuse; tumors; anticoagulant medications; stress ulcers; esophagitis or gastritis.; etc.\",\n",
            "      \"Symptoms\": \"hematemesis (vomiting of red blood ...\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "======================================================================\n",
            "LOADING ALL KNOWLEDGE GRAPH FILES\n",
            "======================================================================\n",
            "\n",
            "Loading knowledge graph files from: ./DiReCT_Knowledge_Graph\n",
            "\n",
            "======================================================================\n",
            "KNOWLEDGE GRAPH SUMMARY\n",
            "======================================================================\n",
            "\n",
            "Total knowledge graph files loaded: 24\n",
            "\n",
            "Loaded Diagnoses:\n",
            "1. Upper Gastrointestinal Bleeding (8 keys, 1074 chars)\n",
            "2. Acute Coronary Syndrome (16 keys, 1052 chars)\n",
            "3. Heart Failure (17 keys, 1727 chars)\n",
            "4. Multiple Sclerosis (16 keys, 3671 chars)\n",
            "5. Stroke (11 keys, 2821 chars)\n",
            "6. Adrenal Insufficiency (13 keys, 2593 chars)\n",
            "7. Epilepsy (11 keys, 2003 chars)\n",
            "8. Hyperlipidemia (8 keys, 1049 chars)\n",
            "9. Cardiomyopathy (14 keys, 2495 chars)\n",
            "10. Hypertension (8 keys, 656 chars)\n",
            "11. Tuberculosis (11 keys, 2631 chars)\n",
            "12. Aortic Dissection (10 keys, 831 chars)\n",
            "13. Peptic Ulcer Disease (10 keys, 1567 chars)\n",
            "14. COPD (17 keys, 1038 chars)\n",
            "15. Pneumonia (13 keys, 1985 chars)\n",
            "16. Migraine (10 keys, 1511 chars)\n",
            "17. Pulmonary Embolism (15 keys, 3338 chars)\n",
            "18. Gastro-oesophageal Reflux Disease (8 keys, 1896 chars)\n",
            "19. Diabetes (17 keys, 3131 chars)\n",
            "20. Alzheimer (8 keys, 2503 chars)\n",
            "21. Asthma (19 keys, 2542 chars)\n",
            "22. Thyroid Disease (21 keys, 3748 chars)\n",
            "23. Atrial Fibrillation (10 keys, 1046 chars)\n",
            "24. Pituitary Disease (13 keys, 2110 chars)\n",
            "\n",
            "Total files: 24\n",
            "Total keys: 304\n",
            "Avg keys per file: 12.7\n",
            "Total text length: 49,018\n",
            "\n",
            "SAMPLE KNOWLEDGE GRAPH ENTRY\n",
            "\n",
            "Diagnosis: Upper Gastrointestinal Bleeding\n",
            "Source: Upper Gastrointestinal Bleeding.json\n",
            "Number of keys: 8\n",
            "\n",
            "Content Preview:\n",
            "----------------------------------------------------------------------\n",
            "• diagnostic\n",
            "  • Suspected Upper Gastrointestinal Bleeding\n",
            "    • Upper Gastrointestinal Bleeding\n",
            "• knowledge\n",
            "  • Suspected Upper Gastrointestinal Bleeding\n",
            "    • Risk Factors\n",
            "      - peptic ulcers; prolonged use of nonsteroidal anti-inflammatory drugs (NSAIDs); Helicobacter pylori infection; esophageal varices; alcohol abuse; tumors; anticoagulant medications; stress ulcers; esophagitis or gastritis.; etc.\n",
            "    • Symptoms\n",
            "      - hematemesis (vomiting of red blood or coffee-grounds material); melena (black, tarry stool), or hematochezia (passage of red or maroon material per rec-tum); anemia; Hemorrhagic peripheral circulatory collapse(dizziness, palpitations, fatigue, fainting when standing up suddenly from a flat position, cold sensation of the limbs, increased heart rate, and low blood pr\n",
            "\n",
            "... (content continues)\n",
            "\n",
            "======================================================================\n",
            "DATASET COMPARISON SUMMARY\n",
            "======================================================================\n",
            "df_notes missing or empty. Cannot compare.\n",
            "\n",
            "DATA LOADING COMPLETE\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "New\n"
      ],
      "metadata": {
        "id": "amKgV4vxMAyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# STANDARDIZE DATAFRAME\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STANDARDIZING DATA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Use df_notes_v2 (input1-6 format) as the standard\n",
        "df_notes = df_notes_v2.copy()\n",
        "print(f\"✓ Standardized on df_notes_v2\")\n",
        "print(f\"✓ Total clinical notes: {len(df_notes)}\")\n",
        "print(f\"✓ Total knowledge graph entries: {len(df_kg)}\")\n",
        "\n",
        "# Display sample\n",
        "print(\"\\nSample clinical note:\")\n",
        "print(f\"  Note ID: {df_notes.iloc[0]['note_id']}\")\n",
        "print(f\"  Diagnosis: {df_notes.iloc[0]['diagnosis']}\")\n",
        "print(f\"  Text preview: {df_notes.iloc[0]['full_text'][:100]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXNK1PEiL_6Z",
        "outputId": "1f18a34a-d03c-4cce-b4d2-d927df42695c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "STANDARDIZING DATA\n",
            "======================================================================\n",
            "✓ Standardized on df_notes_v2\n",
            "✓ Total clinical notes: 511\n",
            "✓ Total knowledge graph entries: 24\n",
            "\n",
            "Sample clinical note:\n",
            "  Note ID: 18427803-DS-5.json\n",
            "  Diagnosis: Migraine With Aura\n",
            "  Text preview: CHIEF COMPLAINT: Difficulty producing speech \n",
            "\n",
            "HISTORY OF PRESENT ILLNESS: Patient woke up at about ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CREATE TEXT CHUNKS FROM CLINICAL NOTES AND KNOWLEDGE GRAPH\n",
        "# =============================================================================\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CREATING TEXT CHUNKS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Initialize text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \"],\n",
        "    length_function=len\n",
        ")\n",
        "\n",
        "print(\"✓ Text splitter initialized\")\n",
        "print(f\"  Chunk size: 500\")\n",
        "print(f\"  Chunk overlap: 50\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Create chunks from clinical notes\n",
        "# ---------------------------------------------\n",
        "def create_chunks_from_notes(df_notes):\n",
        "    \"\"\"Create chunks from clinical notes with enhanced metadata\"\"\"\n",
        "    all_chunks = []\n",
        "\n",
        "    print(\"\\nProcessing clinical notes...\")\n",
        "    for idx, row in df_notes.iterrows():\n",
        "        note_id = row['note_id']\n",
        "        diagnosis = row.get('diagnosis', 'Unknown')\n",
        "        full_text = row['full_text']\n",
        "\n",
        "        # Create enhanced text with metadata header\n",
        "        enhanced_text = f\"NOTE_ID: {note_id}\\nDIAGNOSIS: {diagnosis}\\n\\n{full_text}\"\n",
        "\n",
        "        # Split into chunks\n",
        "        chunks = text_splitter.create_documents(\n",
        "            texts=[enhanced_text],\n",
        "            metadatas=[{\n",
        "                'note_id': note_id,\n",
        "                'diagnosis': diagnosis,\n",
        "                'type': 'clinical_note'\n",
        "            }]\n",
        "        )\n",
        "\n",
        "        # Add chunk indices\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            chunk.metadata['chunk_index'] = i\n",
        "\n",
        "        all_chunks.extend(chunks)\n",
        "\n",
        "        if (idx + 1) % 100 == 0:\n",
        "            print(f\"  Processed {idx + 1}/{len(df_notes)} notes...\")\n",
        "\n",
        "    print(f\"✓ Created {len(all_chunks)} chunks from clinical notes\")\n",
        "    return all_chunks\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Create chunks from knowledge graph\n",
        "# ---------------------------------------------\n",
        "def create_chunks_from_kg(df_kg):\n",
        "    \"\"\"Create chunks from knowledge graph data\"\"\"\n",
        "    kg_chunks = []\n",
        "\n",
        "    print(\"\\nProcessing knowledge graph...\")\n",
        "    for idx, row in df_kg.iterrows():\n",
        "        diagnosis = row['diagnosis']\n",
        "        text_content = row['text_content']\n",
        "\n",
        "        # Add header\n",
        "        enhanced_text = f\"KNOWLEDGE: {diagnosis}\\n\\n{text_content}\"\n",
        "\n",
        "        chunks = text_splitter.create_documents(\n",
        "            texts=[enhanced_text],\n",
        "            metadatas=[{\n",
        "                'diagnosis': diagnosis,\n",
        "                'type': 'knowledge_graph',\n",
        "                'source_file': row['source_file']\n",
        "            }]\n",
        "        )\n",
        "\n",
        "        # Add chunk indices\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            chunk.metadata['chunk_index'] = i\n",
        "\n",
        "        kg_chunks.extend(chunks)\n",
        "\n",
        "    print(f\"✓ Created {len(kg_chunks)} chunks from knowledge graph\")\n",
        "    return kg_chunks\n",
        "\n",
        "\n",
        "# Create all chunks\n",
        "clinical_chunks = create_chunks_from_notes(df_notes)\n",
        "kg_chunks = create_chunks_from_kg(df_kg)\n",
        "\n",
        "# Combine all chunks\n",
        "all_combined_chunks = clinical_chunks + kg_chunks\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"CHUNK CREATION SUMMARY\")\n",
        "print(\"-\"*70)\n",
        "print(f\"Clinical note chunks:     {len(clinical_chunks)}\")\n",
        "print(f\"Knowledge graph chunks:   {len(kg_chunks)}\")\n",
        "print(f\"Total combined chunks:    {len(all_combined_chunks)}\")\n",
        "\n",
        "# Display sample chunks\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"SAMPLE CHUNKS\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "print(\"\\nClinical Note Chunk:\")\n",
        "print(f\"Content: {clinical_chunks[0].page_content[:200]}...\")\n",
        "print(f\"Metadata: {clinical_chunks[0].metadata}\")\n",
        "\n",
        "if kg_chunks:\n",
        "    print(\"\\nKnowledge Graph Chunk:\")\n",
        "    print(f\"Content: {kg_chunks[0].page_content[:200]}...\")\n",
        "    print(f\"Metadata: {kg_chunks[0].metadata}\")\n",
        "\n",
        "print(\"\\n✓ Chunk creation complete\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E75E5smKMFlv",
        "outputId": "14453e16-a04b-42df-cd9f-6d1ac6f843e6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "CREATING TEXT CHUNKS\n",
            "======================================================================\n",
            "✓ Text splitter initialized\n",
            "  Chunk size: 500\n",
            "  Chunk overlap: 50\n",
            "\n",
            "Processing clinical notes...\n",
            "  Processed 100/511 notes...\n",
            "  Processed 200/511 notes...\n",
            "  Processed 300/511 notes...\n",
            "  Processed 400/511 notes...\n",
            "  Processed 500/511 notes...\n",
            "✓ Created 5582 chunks from clinical notes\n",
            "\n",
            "Processing knowledge graph...\n",
            "✓ Created 159 chunks from knowledge graph\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "CHUNK CREATION SUMMARY\n",
            "----------------------------------------------------------------------\n",
            "Clinical note chunks:     5582\n",
            "Knowledge graph chunks:   159\n",
            "Total combined chunks:    5741\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "SAMPLE CHUNKS\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Clinical Note Chunk:\n",
            "Content: NOTE_ID: 18427803-DS-5.json\n",
            "DIAGNOSIS: Migraine With Aura\n",
            "\n",
            "CHIEF COMPLAINT: Difficulty producing speech \n",
            "\n",
            "HISTORY OF PRESENT ILLNESS: Patient woke up at about 0900 AM. Patient felt that she had a migr...\n",
            "Metadata: {'note_id': '18427803-DS-5.json', 'diagnosis': 'Migraine With Aura', 'type': 'clinical_note', 'chunk_index': 0}\n",
            "\n",
            "Knowledge Graph Chunk:\n",
            "Content: KNOWLEDGE: Upper Gastrointestinal Bleeding...\n",
            "Metadata: {'diagnosis': 'Upper Gastrointestinal Bleeding', 'type': 'knowledge_graph', 'source_file': 'Upper Gastrointestinal Bleeding.json', 'chunk_index': 0}\n",
            "\n",
            "✓ Chunk creation complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# INITIALIZE EMBEDDINGS AND CREATE FAISS VECTOR STORE\n",
        "# =============================================================================\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"INITIALIZING EMBEDDINGS AND VECTOR STORE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Initialize embedding model\n",
        "# ---------------------------------------------\n",
        "print(\"\\nLoading embedding model: sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_kwargs={'device': 'cpu'},\n",
        "    encode_kwargs={'normalize_embeddings': True}\n",
        ")\n",
        "\n",
        "print(\"✓ Embedding model loaded successfully\")\n",
        "print(\"  Model: sentence-transformers/all-MiniLM-L6-v2\")\n",
        "print(\"  Embedding dimension: 384\")\n",
        "\n",
        "# Test embeddings\n",
        "test_text = \"Patient has migraine with aura\"\n",
        "test_embedding = embeddings.embed_query(test_text)\n",
        "print(f\"  Test embedding length: {len(test_embedding)}\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Create FAISS vector store\n",
        "# ---------------------------------------------\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"Creating FAISS vector store...\")\n",
        "print(\"This may take 2-5 minutes depending on data size...\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "# Create vector store from all chunks\n",
        "vectorstore = FAISS.from_documents(\n",
        "    documents=all_combined_chunks,\n",
        "    embedding=embeddings\n",
        ")\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\n✓ Vector store created successfully\")\n",
        "print(f\"  Time taken: {elapsed_time:.2f} seconds\")\n",
        "print(f\"  Total vectors: {vectorstore.index.ntotal}\")\n",
        "print(f\"  Clinical note vectors: ~{len(clinical_chunks)}\")\n",
        "print(f\"  Knowledge graph vectors: ~{len(kg_chunks)}\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Test vector store with sample queries\n",
        "# ---------------------------------------------\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"TESTING VECTOR STORE\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "test_queries = [\n",
        "    \"migraine symptoms\",\n",
        "    \"patient 18427803-DS-5\",\n",
        "    \"headache treatment\"\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    print(f\"\\nTest query: '{query}'\")\n",
        "    results = vectorstore.similarity_search(query, k=3)\n",
        "    print(f\"  Retrieved {len(results)} documents:\")\n",
        "    for i, doc in enumerate(results):\n",
        "        doc_type = doc.metadata.get('type', 'unknown')\n",
        "        diagnosis = doc.metadata.get('diagnosis', 'N/A')\n",
        "        note_id = doc.metadata.get('note_id', 'N/A')\n",
        "        preview = doc.page_content[:80].replace('\\n', ' ')\n",
        "\n",
        "        if doc_type == 'clinical_note':\n",
        "            print(f\"    {i+1}. [Clinical] {note_id} | {diagnosis}\")\n",
        "        else:\n",
        "            print(f\"    {i+1}. [Knowledge] {diagnosis}\")\n",
        "        print(f\"       {preview}...\")\n",
        "\n",
        "print(\"\\n✓ Vector store is working correctly\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DGEK_GJRNaA",
        "outputId": "b1b21fc2-2d29-4791-e99f-46f44b3368ac"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "INITIALIZING EMBEDDINGS AND VECTOR STORE\n",
            "======================================================================\n",
            "\n",
            "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Embedding model loaded successfully\n",
            "  Model: sentence-transformers/all-MiniLM-L6-v2\n",
            "  Embedding dimension: 384\n",
            "  Test embedding length: 384\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Creating FAISS vector store...\n",
            "This may take 2-5 minutes depending on data size...\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "✓ Vector store created successfully\n",
            "  Time taken: 284.11 seconds\n",
            "  Total vectors: 5741\n",
            "  Clinical note vectors: ~5582\n",
            "  Knowledge graph vectors: ~159\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "TESTING VECTOR STORE\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Test query: 'migraine symptoms'\n",
            "  Retrieved 3 documents:\n",
            "    1. [Knowledge] Migraine\n",
            "       • Migraine Without Aura     - Headache attacks often begin suddenly, with no app...\n",
            "    2. [Knowledge] Migraine\n",
            "       KNOWLEDGE: Migraine...\n",
            "    3. [Clinical] 18805216-DS-21.json | Migraine With Aura\n",
            "       NOTE_ID: 18805216-DS-21.json DIAGNOSIS: Migraine With Aura  CHIEF COMPLAINT: N/A...\n",
            "\n",
            "Test query: 'patient 18427803-DS-5'\n",
            "  Retrieved 3 documents:\n",
            "    1. [Clinical] 19141642-DS-10.json | Hypertension\n",
            "       HISTORY OF PRESENT ILLNESS: 58 year old female with pmhx of autoimmune hemolytic...\n",
            "    2. [Clinical] 18833652-DS-11.json | Ischemic Stroke\n",
            "       She went to bed yesterday evening around 2200 feeling well. She was found abnorm...\n",
            "    3. [Clinical] 12759698-DS-3.json | Low-risk PE\n",
            "       HISTORY OF PRESENT ILLNESS: Patient seen and examined, agree with house officer ...\n",
            "\n",
            "Test query: 'headache treatment'\n",
            "  Retrieved 3 documents:\n",
            "    1. [Clinical] 11573828-DS-4.json | Pituitary Macroadenomas\n",
            "       CHIEF COMPLAINT: elective admit  HISTORY OF PRESENT ILLNESS: s is a gentleman wi...\n",
            "    2. [Clinical] 18427803-DS-5.json | Migraine With Aura\n",
            "       . This helped with headache, but not aphasia....\n",
            "    3. [Clinical] 17185323-DS-14.json | Migraine Without Aura\n",
            "       .  Headache attacks often begin suddenly, with no apparent warning period...\n",
            "\n",
            "✓ Vector store is working correctly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# INITIALIZE LANGUAGE MODEL (LLM)\n",
        "# =============================================================================\n",
        "\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"INITIALIZING LANGUAGE MODEL\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Model configuration\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "print(f\"\\nLoading model: {MODEL_NAME}\")\n",
        "print(\"This may take 3-5 minutes on first run...\")\n",
        "print(\"(Model will be cached for future runs)\")\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Load tokenizer\n",
        "# ---------------------------------------------\n",
        "print(\"\\nStep 1: Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Set pad token if not present\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"✓ Tokenizer loaded\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Load model\n",
        "# ---------------------------------------------\n",
        "print(\"\\nStep 2: Loading model...\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\",\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "print(\"✓ Model loaded\")\n",
        "print(f\"  Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
        "print(f\"  Dtype: {model.dtype}\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Create text generation pipeline\n",
        "# ---------------------------------------------\n",
        "print(\"\\nStep 3: Creating generation pipeline...\")\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.15,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "print(\"✓ Pipeline created\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Wrap in LangChain\n",
        "# ---------------------------------------------\n",
        "print(\"\\nStep 4: Wrapping in LangChain...\")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "print(\"✓ LLM ready for use\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Test LLM\n",
        "# ---------------------------------------------\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"TESTING LLM\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "test_prompt = \"What are the symptoms of migraine? Answer briefly:\"\n",
        "print(f\"\\nTest prompt: '{test_prompt}'\")\n",
        "print(\"Generating response...\\n\")\n",
        "\n",
        "test_output = llm.invoke(test_prompt)\n",
        "\n",
        "print(f\"Response: {test_output[:200]}...\")\n",
        "\n",
        "print(\"\\n✓ LLM is working correctly\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ALL COMPONENTS INITIALIZED\")\n",
        "print(\"=\"*70)\n",
        "print(f\"✓ Embeddings:    Ready ({embeddings.model_name})\")\n",
        "print(f\"✓ Vector Store:  Ready ({vectorstore.index.ntotal} vectors)\")\n",
        "print(f\"✓ LLM:           Ready ({MODEL_NAME})\")\n",
        "print(f\"✓ Chunks:        Ready ({len(all_combined_chunks)} chunks)\")\n",
        "print(\"\\nSystem ready for RAG pipeline creation!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fp_O-w3kMOK0",
        "outputId": "eab0d06e-1dd7-4244-bb02-5f2bef84c1f5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "INITIALIZING LANGUAGE MODEL\n",
            "======================================================================\n",
            "\n",
            "Loading model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
            "This may take 3-5 minutes on first run...\n",
            "(Model will be cached for future runs)\n",
            "\n",
            "Step 1: Loading tokenizer...\n",
            "✓ Tokenizer loaded\n",
            "\n",
            "Step 2: Loading model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Model loaded\n",
            "  Device: GPU\n",
            "  Dtype: torch.float16\n",
            "\n",
            "Step 3: Creating generation pipeline...\n",
            "✓ Pipeline created\n",
            "\n",
            "Step 4: Wrapping in LangChain...\n",
            "✓ LLM ready for use\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "TESTING LLM\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Test prompt: 'What are the symptoms of migraine? Answer briefly:'\n",
            "Generating response...\n",
            "\n",
            "Response: What are the symptoms of migraine? Answer briefly:\n",
            "- Migraine headaches can cause severe pain and discomfort on one side of the head or face, accompanied by sensitivity to light, sound, smells, tastes...\n",
            "\n",
            "✓ LLM is working correctly\n",
            "\n",
            "======================================================================\n",
            "ALL COMPONENTS INITIALIZED\n",
            "======================================================================\n",
            "✓ Embeddings:    Ready (sentence-transformers/all-MiniLM-L6-v2)\n",
            "✓ Vector Store:  Ready (5741 vectors)\n",
            "✓ LLM:           Ready (TinyLlama/TinyLlama-1.1B-Chat-v1.0)\n",
            "✓ Chunks:        Ready (5741 chunks)\n",
            "\n",
            "System ready for RAG pipeline creation!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# VERIFY ALL COMPONENTS BEFORE CREATING RETRIEVER\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SYSTEM COMPONENT VERIFICATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "components_status = {\n",
        "    'df_notes': 'df_notes' in globals() and not df_notes.empty,\n",
        "    'df_kg': 'df_kg' in globals() and not df_kg.empty,\n",
        "    'all_combined_chunks': 'all_combined_chunks' in globals() and len(all_combined_chunks) > 0,\n",
        "    'embeddings': 'embeddings' in globals(),\n",
        "    'vectorstore': 'vectorstore' in globals() and hasattr(vectorstore, 'index'),\n",
        "    'llm': 'llm' in globals()\n",
        "}\n",
        "\n",
        "print(\"\\nComponent Status:\")\n",
        "for component, status in components_status.items():\n",
        "    status_icon = \"✓\" if status else \"✗\"\n",
        "    print(f\"  {status_icon} {component}: {'Ready' if status else 'MISSING'}\")\n",
        "\n",
        "if all(components_status.values()):\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"✓ ALL COMPONENTS READY - PROCEEDING TO RETRIEVER CREATION\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(\"\\nDetailed Statistics:\")\n",
        "    print(f\"  Clinical Notes:        {len(df_notes)}\")\n",
        "    print(f\"  Knowledge Graph:       {len(df_kg)}\")\n",
        "    print(f\"  Total Chunks:          {len(all_combined_chunks)}\")\n",
        "    print(f\"  Vector Store Size:     {vectorstore.index.ntotal}\")\n",
        "    print(f\"  Embedding Model:       {embeddings.model_name}\")\n",
        "    print(f\"  LLM Model:             TinyLlama-1.1B-Chat\")\n",
        "else:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"✗ ERROR: MISSING COMPONENTS\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\nPlease run the previous cells to initialize:\")\n",
        "    for component, status in components_status.items():\n",
        "        if not status:\n",
        "            print(f\"  - {component}\")\n",
        "\n",
        "    raise RuntimeError(\"Cannot proceed without all components initialized\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIvR6NbvMTDR",
        "outputId": "dbd89819-2975-46fb-8278-c35c3e37913f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "SYSTEM COMPONENT VERIFICATION\n",
            "======================================================================\n",
            "\n",
            "Component Status:\n",
            "  ✓ df_notes: Ready\n",
            "  ✓ df_kg: Ready\n",
            "  ✓ all_combined_chunks: Ready\n",
            "  ✓ embeddings: Ready\n",
            "  ✓ vectorstore: Ready\n",
            "  ✓ llm: Ready\n",
            "\n",
            "======================================================================\n",
            "✓ ALL COMPONENTS READY - PROCEEDING TO RETRIEVER CREATION\n",
            "======================================================================\n",
            "\n",
            "Detailed Statistics:\n",
            "  Clinical Notes:        511\n",
            "  Knowledge Graph:       24\n",
            "  Total Chunks:          5741\n",
            "  Vector Store Size:     5741\n",
            "  Embedding Model:       sentence-transformers/all-MiniLM-L6-v2\n",
            "  LLM Model:             TinyLlama-1.1B-Chat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "before"
      ],
      "metadata": {
        "id": "ZPIHsDHCMCZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# FINAL DEFINITIVE FIX - Direct Metadata Filtering (CLEAN VERSION)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINAL FIX: DIRECT SEARCH BY METADATA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 1: Inspect exact metadata format\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\nInspecting metadata format...\")\n",
        "\n",
        "# Get actual chunks for our target patient\n",
        "target_chunks = [\n",
        "    c for c in all_combined_chunks\n",
        "    if '18427803-DS-5' in c.metadata.get('note_id', '')\n",
        "]\n",
        "\n",
        "if target_chunks:\n",
        "    sample = target_chunks[0]\n",
        "    print(\"\\nTarget patient chunks found.\")\n",
        "    print(f\"   Total chunks: {len(target_chunks)}\")\n",
        "    print(\"   Metadata format:\")\n",
        "    print(f\"     note_id: '{sample.metadata.get('note_id')}'\")\n",
        "    print(f\"     type: '{sample.metadata.get('type')}'\")\n",
        "    print(f\"     diagnosis: '{sample.metadata.get('diagnosis')}'\")\n",
        "    print(\"\\n   Content preview:\")\n",
        "    print(f\"     {sample.page_content[:150]}\")\n",
        "\n",
        "    EXACT_NOTE_ID = sample.metadata.get('note_id')\n",
        "    print(f\"\\n   Exact note_id format: '{EXACT_NOTE_ID}'\")\n",
        "else:\n",
        "    print(\"ERROR: Target chunks not found in all_combined_chunks\")\n",
        "    EXACT_NOTE_ID = None\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 2: Create Working Retriever with Direct Filtering\n",
        "# =============================================================================\n",
        "\n",
        "from langchain.schema import BaseRetriever, Document\n",
        "from typing import List\n",
        "import re\n",
        "\n",
        "class WorkingRetriever(BaseRetriever):\n",
        "    \"\"\"\n",
        "    Retriever that uses direct list filtering for guaranteed deterministic lookup.\n",
        "    \"\"\"\n",
        "    vectorstore: object\n",
        "    all_chunks: List[Document]\n",
        "    k: int = 5\n",
        "\n",
        "    class Config:\n",
        "        arbitrary_types_allowed = True\n",
        "\n",
        "    def __init__(self, vectorstore, all_chunks, k=5):\n",
        "        super().__init__(\n",
        "            vectorstore=vectorstore,\n",
        "            all_chunks=all_chunks,\n",
        "            k=k\n",
        "        )\n",
        "\n",
        "    def extract_note_id(self, query):\n",
        "        \"\"\"Extract note_id pattern from query.\"\"\"\n",
        "        pattern = r'\\b(\\d{8}-[A-Z]{2}-\\d+)\\b'\n",
        "        match = re.search(pattern, query)\n",
        "        return match.group(1) if match else None\n",
        "\n",
        "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
        "        \"\"\"Deterministic filtering, fallback to semantic search.\"\"\"\n",
        "        note_id_pattern = self.extract_note_id(query)\n",
        "\n",
        "        if note_id_pattern:\n",
        "            print(f\"Searching for note_id pattern: {note_id_pattern}\")\n",
        "\n",
        "            matching_chunks = [\n",
        "                chunk for chunk in self.all_chunks\n",
        "                if note_id_pattern in chunk.metadata.get('note_id', '')\n",
        "            ]\n",
        "\n",
        "            if matching_chunks:\n",
        "                print(f\"   Found {len(matching_chunks)} chunks for this patient.\")\n",
        "\n",
        "                if len(matching_chunks) > self.k:\n",
        "                    ranked = self.vectorstore.similarity_search(query, k=50)\n",
        "                    matching_note_ids = set([c.metadata.get('note_id') for c in matching_chunks])\n",
        "\n",
        "                    ranked_matches = [\n",
        "                        doc for doc in ranked\n",
        "                        if doc.metadata.get('note_id') in matching_note_ids\n",
        "                    ]\n",
        "\n",
        "                    if ranked_matches:\n",
        "                        return ranked_matches[:self.k]\n",
        "\n",
        "                sorted_matches = sorted(\n",
        "                    matching_chunks,\n",
        "                    key=lambda x: x.metadata.get('chunk_index', 0)\n",
        "                )\n",
        "                return sorted_matches[:self.k]\n",
        "\n",
        "            else:\n",
        "                print(\"   No chunks found matching that patient ID.\")\n",
        "\n",
        "        # Fallback mode\n",
        "        print(\"Using semantic search instead.\")\n",
        "        return self.vectorstore.similarity_search(query, k=self.k)\n",
        "\n",
        "    async def _aget_relevant_documents(self, query: str) -> List[Document]:\n",
        "        return self._get_relevant_documents(query)\n",
        "\n",
        "\n",
        "# Instantiate retriever\n",
        "print(\"\\nCreating working retriever...\")\n",
        "working_retriever = WorkingRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    all_chunks=all_combined_chunks,\n",
        "    k=5\n",
        ")\n",
        "print(\"Working retriever created.\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 3: Test Direct Filtering\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TESTING DIRECT FILTERING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "test_query = \"What is the chief complaint for patient 18427803-DS-5?\"\n",
        "print(f\"\\nQuery: {test_query}\")\n",
        "\n",
        "retrieved_docs = working_retriever._get_relevant_documents(test_query)\n",
        "\n",
        "print(f\"\\nRetrieved {len(retrieved_docs)} documents:\")\n",
        "for i, doc in enumerate(retrieved_docs):\n",
        "    note_id = doc.metadata.get('note_id')\n",
        "    chunk_idx = doc.metadata.get('chunk_index')\n",
        "    preview = doc.page_content[:100].replace('\\n', ' ')\n",
        "    print(f\"   {i+1}. {note_id} [chunk {chunk_idx}]\")\n",
        "    print(f\"      {preview}...\")\n",
        "\n",
        "if any('18427803-DS-5' in doc.metadata.get('note_id', '') for doc in retrieved_docs):\n",
        "    print(\"\\nSUCCESS: Target patient retrieved.\")\n",
        "else:\n",
        "    print(\"\\nFAILURE: Target patient not retrieved.\")\n",
        "    for doc in retrieved_docs:\n",
        "        print(f\"   Retrieved: {doc.metadata.get('note_id')}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 4: Rebuild QA Chain with Working Retriever\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"REBUILDING QA CHAIN WITH WORKING RETRIEVER\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "CLEAN_PROMPT = \"\"\"You are a clinical assistant. Use ONLY the context below to answer.\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUESTION: {question}\n",
        "\n",
        "ANSWER:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=CLEAN_PROMPT,\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "final_qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=working_retriever,\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "print(\"Final QA chain ready.\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 5: Final Comprehensive Testing\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINAL COMPREHENSIVE TEST\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "def final_test(query, expected_pattern=None):\n",
        "    print(\"\\n\" + \"-\"*70)\n",
        "    print(f\"Query: {query}\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    result = final_qa_chain.invoke({\"query\": query})\n",
        "    retrieved_notes = [doc.metadata.get('note_id') for doc in result['source_documents']]\n",
        "\n",
        "    if expected_pattern:\n",
        "        found = any(expected_pattern in note for note in retrieved_notes)\n",
        "        status = \"SUCCESS\" if found else \"FAILED\"\n",
        "        print(f\"\\n{status}: Expected '{expected_pattern}'\")\n",
        "        print(f\"Retrieved: {retrieved_notes[:3]}\")\n",
        "\n",
        "    answer = result[\"result\"].strip()\n",
        "    if \"ANSWER:\" in answer:\n",
        "        answer = answer.split(\"ANSWER:\")[-1].strip()\n",
        "\n",
        "    print(\"\\nGenerated Answer:\")\n",
        "    print(answer[:300])\n",
        "\n",
        "    print(\"\\nSource Documents:\")\n",
        "    for i, doc in enumerate(result['source_documents'][:3]):\n",
        "        note_id = doc.metadata.get('note_id')\n",
        "        preview = doc.page_content[:80].replace('\\n', ' ')\n",
        "        print(f\"   {i+1}. {note_id}\")\n",
        "        print(f\"      {preview}...\")\n",
        "\n",
        "    return result\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TEST 1: Target Patient (18427803-DS-5)\")\n",
        "final_test(\"What is the chief complaint for patient 18427803-DS-5?\", expected_pattern=\"18427803-DS-5\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TEST 2: Detailed Patient Query\")\n",
        "final_test(\"What are all the clinical findings for patient 18427803-DS-5?\", expected_pattern=\"18427803-DS-5\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TEST 3: General Diagnostic\")\n",
        "final_test(\"What are the key features of migraine with aura?\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SYSTEM SUMMARY\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINAL SYSTEM STATUS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "target_test = working_retriever._get_relevant_documents(\"patient 18427803-DS-5\")\n",
        "success = any('18427803-DS-5' in doc.metadata.get('note_id', '') for doc in target_test)\n",
        "\n",
        "status = f\"\"\"\n",
        "SYSTEM STATUS:\n",
        "   • Vector Store: {vectorstore.index.ntotal} vectors\n",
        "   • All Chunks: {len(all_combined_chunks)} available\n",
        "   • Direct Filtering: {'Working' if success else 'Needs Debugging'}\n",
        "   • Target Patient Retrievable: {'YES' if success else 'NO'}\n",
        "\n",
        "{'System ready for use.' if success else 'Check chunk metadata format.'}\n",
        "\"\"\"\n",
        "\n",
        "print(status)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5Rge-2zvZPc",
        "outputId": "97fbb58b-16b3-4eef-8657-392338c315bd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "FINAL FIX: DIRECT SEARCH BY METADATA\n",
            "======================================================================\n",
            "\n",
            "Inspecting metadata format...\n",
            "\n",
            "Target patient chunks found.\n",
            "   Total chunks: 14\n",
            "   Metadata format:\n",
            "     note_id: '18427803-DS-5.json'\n",
            "     type: 'clinical_note'\n",
            "     diagnosis: 'Migraine With Aura'\n",
            "\n",
            "   Content preview:\n",
            "     NOTE_ID: 18427803-DS-5.json\n",
            "DIAGNOSIS: Migraine With Aura\n",
            "\n",
            "CHIEF COMPLAINT: Difficulty producing speech \n",
            "\n",
            "HISTORY OF PRESENT ILLNESS: Patient woke up \n",
            "\n",
            "   Exact note_id format: '18427803-DS-5.json'\n",
            "\n",
            "Creating working retriever...\n",
            "Working retriever created.\n",
            "\n",
            "======================================================================\n",
            "TESTING DIRECT FILTERING\n",
            "======================================================================\n",
            "\n",
            "Query: What is the chief complaint for patient 18427803-DS-5?\n",
            "Searching for note_id pattern: 18427803-DS-5\n",
            "   Found 14 chunks for this patient.\n",
            "\n",
            "Retrieved 5 documents:\n",
            "   1. 18427803-DS-5.json [chunk 0]\n",
            "      NOTE_ID: 18427803-DS-5.json DIAGNOSIS: Migraine With Aura  CHIEF COMPLAINT: Difficulty producing spe...\n",
            "   2. 18427803-DS-5.json [chunk 1]\n",
            "      Patient's husband called ___ and talked to nurse in primary...\n",
            "   3. 18427803-DS-5.json [chunk 2]\n",
            "      physicians office. Nurse told her to call ___ and ambulance. Patient was taken to ED at ___. Present...\n",
            "   4. 18427803-DS-5.json [chunk 3]\n",
            "      . This helped with headache, but not aphasia....\n",
            "   5. 18427803-DS-5.json [chunk 4]\n",
            "      Patient's husband reports that verbal output has improved only mildly since symptom onset. Patient's...\n",
            "\n",
            "SUCCESS: Target patient retrieved.\n",
            "\n",
            "======================================================================\n",
            "REBUILDING QA CHAIN WITH WORKING RETRIEVER\n",
            "======================================================================\n",
            "Final QA chain ready.\n",
            "\n",
            "======================================================================\n",
            "FINAL COMPREHENSIVE TEST\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "TEST 1: Target Patient (18427803-DS-5)\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Query: What is the chief complaint for patient 18427803-DS-5?\n",
            "----------------------------------------------------------------------\n",
            "Searching for note_id pattern: 18427803-DS-5\n",
            "   Found 14 chunks for this patient.\n",
            "\n",
            "SUCCESS: Expected '18427803-DS-5'\n",
            "Retrieved: ['18427803-DS-5.json', '18427803-DS-5.json', '18427803-DS-5.json']\n",
            "\n",
            "Generated Answer:\n",
            "The chief complaint for patient 18427803-DS-5 is difficulty producing speech\n",
            "\n",
            "Source Documents:\n",
            "   1. 18427803-DS-5.json\n",
            "      NOTE_ID: 18427803-DS-5.json DIAGNOSIS: Migraine With Aura  CHIEF COMPLAINT: Diff...\n",
            "   2. 18427803-DS-5.json\n",
            "      Patient's husband called ___ and talked to nurse in primary...\n",
            "   3. 18427803-DS-5.json\n",
            "      physicians office. Nurse told her to call ___ and ambulance. Patient was taken t...\n",
            "\n",
            "======================================================================\n",
            "TEST 2: Detailed Patient Query\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Query: What are all the clinical findings for patient 18427803-DS-5?\n",
            "----------------------------------------------------------------------\n",
            "Searching for note_id pattern: 18427803-DS-5\n",
            "   Found 14 chunks for this patient.\n",
            "\n",
            "SUCCESS: Expected '18427803-DS-5'\n",
            "Retrieved: ['18427803-DS-5.json', '18427803-DS-5.json', '18427803-DS-5.json']\n",
            "\n",
            "Generated Answer:\n",
            "All the clinical findings for patient 18427803-DS-5 are listed below:\n",
            "\n",
            "- Difficulty producing speech\n",
            "- Headache and altered quality of speech\n",
            "- Continuous headache (difficulty producing speech)\n",
            "- Altered quality of speech (difficulty producing speech)\n",
            "- Aphasia\n",
            "- Verbal output has improved only mild\n",
            "\n",
            "Source Documents:\n",
            "   1. 18427803-DS-5.json\n",
            "      NOTE_ID: 18427803-DS-5.json DIAGNOSIS: Migraine With Aura  CHIEF COMPLAINT: Diff...\n",
            "   2. 18427803-DS-5.json\n",
            "      Patient's husband called ___ and talked to nurse in primary...\n",
            "   3. 18427803-DS-5.json\n",
            "      physicians office. Nurse told her to call ___ and ambulance. Patient was taken t...\n",
            "\n",
            "======================================================================\n",
            "TEST 3: General Diagnostic\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Query: What are the key features of migraine with aura?\n",
            "----------------------------------------------------------------------\n",
            "Using semantic search instead.\n",
            "\n",
            "Generated Answer:\n",
            "Migraine without aura has no preceding visual abnormality that can be observed beforehand. Instead, it presents itself as an immediate headache following by a persistent painful episode. It does not have any accompanying symptom such as sensory deficit, motor impairment, or seizures. The aura typica\n",
            "\n",
            "Source Documents:\n",
            "   1. 17185323-DS-14.json\n",
            "      NOTE_ID: 17185323-DS-14.json DIAGNOSIS: Migraine Without Aura  CHIEF COMPLAINT: ...\n",
            "   2. 18805216-DS-21.json\n",
            "      NOTE_ID: 18805216-DS-21.json DIAGNOSIS: Migraine With Aura  CHIEF COMPLAINT: N/A...\n",
            "   3. None\n",
            "      • Migraine With Aura     - Headache attacks are preceded by a specific set of sy...\n",
            "\n",
            "======================================================================\n",
            "FINAL SYSTEM STATUS\n",
            "======================================================================\n",
            "Searching for note_id pattern: 18427803-DS-5\n",
            "   Found 14 chunks for this patient.\n",
            "\n",
            "SYSTEM STATUS:\n",
            "   • Vector Store: 5741 vectors\n",
            "   • All Chunks: 5741 available\n",
            "   • Direct Filtering: Working\n",
            "   • Target Patient Retrievable: YES\n",
            "\n",
            "System ready for use.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Integrating the Generative Model"
      ],
      "metadata": {
        "id": "-y3iHxy-v5bD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# FIXED EVALUATION METRICS FOR RAG SYSTEM\n",
        "# =============================================================================\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RAG SYSTEM EVALUATION - FIXED VERSION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ---------------------------------------------\n",
        "# IMPROVED Test Cases with Better Ground Truth\n",
        "# ---------------------------------------------\n",
        "test_cases = [\n",
        "    {\n",
        "        \"query\": \"What is the chief complaint for patient 18427803-DS-5?\",\n",
        "        \"expected\": [\"difficulty producing speech\", \"difficulty\", \"speech\"],\n",
        "        \"note_id\": \"18427803-DS-5\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What is the diagnosis for patient 18427803-DS-5?\",\n",
        "        \"expected\": [\"migraine\", \"migraine with aura\"],\n",
        "        \"note_id\": \"18427803-DS-5\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What are the symptoms of migraine with aura?\",\n",
        "        \"expected\": [\"aura\", \"headache\", \"visual\", \"neurological\"],\n",
        "        \"note_id\": None\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"\\nLoaded Test Cases:\")\n",
        "for i, case in enumerate(test_cases):\n",
        "    print(f\"   {i+1}. {case['query']}\")\n",
        "    print(f\"      Expected: {case['expected']}\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Helper: Improved Answer Cleaning\n",
        "# ---------------------------------------------\n",
        "def clean_llm_response(raw_answer):\n",
        "    \"\"\"Remove prompt artifacts and extract clean answer\"\"\"\n",
        "    answer = raw_answer.strip()\n",
        "\n",
        "    # Remove common prompt artifacts\n",
        "    remove_phrases = [\n",
        "        \"You are a clinical assistant\",\n",
        "        \"Use ONLY the context below to answer\",\n",
        "        \"CONTEXT:\",\n",
        "        \"QUESTION:\",\n",
        "        \"ANSWER:\",\n",
        "        \"Direct Answer:\",\n",
        "        \"NOTE_ID:\"\n",
        "    ]\n",
        "\n",
        "    for phrase in remove_phrases:\n",
        "        if phrase in answer:\n",
        "            # Take content after the phrase\n",
        "            parts = answer.split(phrase)\n",
        "            if len(parts) > 1:\n",
        "                answer = parts[-1].strip()\n",
        "\n",
        "    # Remove repeated context snippets\n",
        "    if \"CHIEF COMPLAINT:\" in answer and answer.index(\"CHIEF COMPLAINT:\") > 100:\n",
        "        answer = answer.split(\"CHIEF COMPLAINT:\")[0].strip()\n",
        "\n",
        "    # Extract just the final answer after all metadata\n",
        "    lines = answer.split('\\n')\n",
        "    clean_lines = [line for line in lines if not line.strip().startswith('NOTE_ID:')]\n",
        "    answer = '\\n'.join(clean_lines).strip()\n",
        "\n",
        "    # If answer is still too long, extract first meaningful sentence\n",
        "    sentences = answer.split('.')\n",
        "    if len(sentences) > 0 and len(answer) > 500:\n",
        "        answer = sentences[0].strip() + '.'\n",
        "\n",
        "    return answer\n",
        "\n",
        "\n",
        "def contains_expected(pred, expected_list):\n",
        "    \"\"\"Check if any expected term is in prediction\"\"\"\n",
        "    pred = pred.lower()\n",
        "    if isinstance(expected_list, str):\n",
        "        expected_list = [expected_list]\n",
        "\n",
        "    for expected in expected_list:\n",
        "        if expected.lower() in pred:\n",
        "            return 1\n",
        "    return 0\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Improved Grounding Check\n",
        "# ---------------------------------------------\n",
        "def check_grounding_improved(answer, sources, threshold=0.4):\n",
        "    \"\"\"Check if answer is grounded in source documents\"\"\"\n",
        "    # Combine all source content\n",
        "    combined_source = \" \".join(doc.page_content.lower() for doc in sources)\n",
        "\n",
        "    # Extract key terms from answer (ignore common words)\n",
        "    answer_lower = answer.lower()\n",
        "    stop_words = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'in', 'on', 'at', 'to', 'for', 'of', 'and', 'or'}\n",
        "    answer_terms = set(answer_lower.split()) - stop_words\n",
        "\n",
        "    if not answer_terms:\n",
        "        return 0\n",
        "\n",
        "    # Check how many answer terms appear in sources\n",
        "    source_terms = set(combined_source.split())\n",
        "    overlap = len(answer_terms & source_terms) / len(answer_terms)\n",
        "\n",
        "    return 1 if overlap >= threshold else 0\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Retrieval Evaluation (Same as before)\n",
        "# ---------------------------------------------\n",
        "def evaluate_retrieval(retriever, test_cases, k=5):\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"EVALUATING RETRIEVAL\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    precision_vals, recall_vals, mrr_vals = [], [], []\n",
        "    patient_accuracy = []\n",
        "\n",
        "    for i, case in enumerate(test_cases):\n",
        "        query = case[\"query\"]\n",
        "        expected = case[\"expected\"]\n",
        "        expected_note = case[\"note_id\"]\n",
        "\n",
        "        print(f\"\\n{i+1}. Query: {query}\")\n",
        "\n",
        "        retrieved_docs = retriever._get_relevant_documents(query)\n",
        "\n",
        "        # Check if correct patient note found\n",
        "        if expected_note:\n",
        "            note_ids = [doc.metadata.get('note_id', '') for doc in retrieved_docs]\n",
        "            found = any(expected_note in n for n in note_ids)\n",
        "            patient_accuracy.append(int(found))\n",
        "            print(f\"   Note Match: {found}\")\n",
        "\n",
        "        # Check relevance\n",
        "        retrieved_texts = [doc.page_content.lower() for doc in retrieved_docs]\n",
        "        relevant_vector = [contains_expected(txt, expected) for txt in retrieved_texts]\n",
        "\n",
        "        precision_vals.append(sum(relevant_vector) / len(relevant_vector))\n",
        "        recall_vals.append(1 if sum(relevant_vector) > 0 else 0)\n",
        "\n",
        "        # Reciprocal Rank\n",
        "        rr = 0\n",
        "        for idx, val in enumerate(relevant_vector):\n",
        "            if val == 1:\n",
        "                rr = 1 / (idx + 1)\n",
        "                break\n",
        "        mrr_vals.append(rr)\n",
        "\n",
        "        print(f\"   Precision: {precision_vals[-1]:.2f}\")\n",
        "        print(f\"   Recall:    {recall_vals[-1]:.2f}\")\n",
        "        print(f\"   MRR:       {rr:.2f}\")\n",
        "\n",
        "    return {\n",
        "        \"precision\": np.mean(precision_vals),\n",
        "        \"recall\": np.mean(recall_vals),\n",
        "        \"mrr\": np.mean(mrr_vals),\n",
        "        \"patient_accuracy\": np.mean(patient_accuracy) if patient_accuracy else None\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# FIXED Generation Evaluation\n",
        "# ---------------------------------------------\n",
        "def evaluate_generation_fixed(qa_chain, test_cases):\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"EVALUATING GENERATION (FIXED)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    accuracy_list, hallucination_list, coherence_list, citation_list = [], [], [], []\n",
        "\n",
        "    for i, case in enumerate(test_cases):\n",
        "        query = case[\"query\"]\n",
        "        expected = case[\"expected\"]\n",
        "\n",
        "        print(f\"\\n{i+1}. Query: {query}\")\n",
        "\n",
        "        result = qa_chain.invoke({\"query\": query})\n",
        "        raw_answer = result[\"result\"]\n",
        "        sources = result[\"source_documents\"]\n",
        "\n",
        "        # Clean the answer\n",
        "        answer = clean_llm_response(raw_answer)\n",
        "\n",
        "        print(f\"   Expected: {expected}\")\n",
        "        print(f\"   Raw Answer (first 100 chars): {raw_answer[:100]}...\")\n",
        "        print(f\"   Cleaned Answer: {answer[:150]}...\")\n",
        "\n",
        "        # Accuracy\n",
        "        acc = contains_expected(answer, expected)\n",
        "        accuracy_list.append(acc)\n",
        "        print(f\"   Accuracy: {acc}\")\n",
        "\n",
        "        # Improved grounding check\n",
        "        grounded = check_grounding_improved(answer, sources)\n",
        "        hallucination_list.append(0 if grounded else 1)\n",
        "        print(f\"   Grounded: {grounded}\")\n",
        "\n",
        "        # Coherence (answer should be substantial)\n",
        "        coherent = len(answer.split()) > 3 and len(answer) < 1000\n",
        "        coherence_list.append(int(coherent))\n",
        "        print(f\"   Coherent: {coherent}\")\n",
        "\n",
        "        # Citation check\n",
        "        contains_expected_source = any(contains_expected(doc.page_content, expected) for doc in sources)\n",
        "        citation_list.append(int(contains_expected_source))\n",
        "        print(f\"   Relevant Sources: {contains_expected_source}\")\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": np.mean(accuracy_list),\n",
        "        \"hallucination_rate\": np.mean(hallucination_list),\n",
        "        \"coherence\": np.mean(coherence_list),\n",
        "        \"citation_quality\": np.mean(citation_list)\n",
        "    }\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# RUN THE FIXED EVALUATION\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RUNNING FIXED EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "retrieval_metrics = evaluate_retrieval(working_retriever, test_cases)\n",
        "generation_metrics = evaluate_generation_fixed(final_qa_chain, test_cases)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# FINAL SUMMARY\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FIXED RESULTS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nRetrieval:\")\n",
        "print(f\"   Precision:        {retrieval_metrics['precision']:.2%}\")\n",
        "print(f\"   Recall:           {retrieval_metrics['recall']:.2%}\")\n",
        "print(f\"   MRR:              {retrieval_metrics['mrr']:.2%}\")\n",
        "print(f\"   Patient Accuracy: {retrieval_metrics['patient_accuracy']:.2%}\")\n",
        "\n",
        "print(\"\\nGeneration:\")\n",
        "print(f\"   Accuracy:           {generation_metrics['accuracy']:.2%}\")\n",
        "print(f\"   Hallucination Rate: {generation_metrics['hallucination_rate']:.2%}\")\n",
        "print(f\"   Coherence:          {generation_metrics['coherence']:.2%}\")\n",
        "print(f\"   Citation Quality:   {generation_metrics['citation_quality']:.2%}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EVALUATION COMPLETE - FIXED VERSION\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "DvmBuyyPHyMl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8aa2d94-a491-4762-f9b2-66962259a07e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "RAG SYSTEM EVALUATION - FIXED VERSION\n",
            "======================================================================\n",
            "\n",
            "Loaded Test Cases:\n",
            "   1. What is the chief complaint for patient 18427803-DS-5?\n",
            "      Expected: ['difficulty producing speech', 'difficulty', 'speech']\n",
            "   2. What is the diagnosis for patient 18427803-DS-5?\n",
            "      Expected: ['migraine', 'migraine with aura']\n",
            "   3. What are the symptoms of migraine with aura?\n",
            "      Expected: ['aura', 'headache', 'visual', 'neurological']\n",
            "\n",
            "======================================================================\n",
            "RUNNING FIXED EVALUATION\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "EVALUATING RETRIEVAL\n",
            "======================================================================\n",
            "\n",
            "1. Query: What is the chief complaint for patient 18427803-DS-5?\n",
            "Searching for note_id pattern: 18427803-DS-5\n",
            "   Found 14 chunks for this patient.\n",
            "   Note Match: True\n",
            "   Precision: 0.20\n",
            "   Recall:    1.00\n",
            "   MRR:       1.00\n",
            "\n",
            "2. Query: What is the diagnosis for patient 18427803-DS-5?\n",
            "Searching for note_id pattern: 18427803-DS-5\n",
            "   Found 14 chunks for this patient.\n",
            "   Note Match: True\n",
            "   Precision: 0.20\n",
            "   Recall:    1.00\n",
            "   MRR:       1.00\n",
            "\n",
            "3. Query: What are the symptoms of migraine with aura?\n",
            "Using semantic search instead.\n",
            "   Precision: 1.00\n",
            "   Recall:    1.00\n",
            "   MRR:       1.00\n",
            "\n",
            "======================================================================\n",
            "EVALUATING GENERATION (FIXED)\n",
            "======================================================================\n",
            "\n",
            "1. Query: What is the chief complaint for patient 18427803-DS-5?\n",
            "Searching for note_id pattern: 18427803-DS-5\n",
            "   Found 14 chunks for this patient.\n",
            "   Expected: ['difficulty producing speech', 'difficulty', 'speech']\n",
            "   Raw Answer (first 100 chars): You are a clinical assistant. Use ONLY the context below to answer.\n",
            "\n",
            "CONTEXT:\n",
            "NOTE_ID: 18427803-DS-5...\n",
            "   Cleaned Answer: The chief complaint for patient 18427803-DS-5 is \"Difficulty producing speech\"...\n",
            "   Accuracy: 1\n",
            "   Grounded: 1\n",
            "   Coherent: True\n",
            "   Relevant Sources: True\n",
            "\n",
            "2. Query: What is the diagnosis for patient 18427803-DS-5?\n",
            "Searching for note_id pattern: 18427803-DS-5\n",
            "   Found 14 chunks for this patient.\n",
            "   Expected: ['migraine', 'migraine with aura']\n",
            "   Raw Answer (first 100 chars): You are a clinical assistant. Use ONLY the context below to answer.\n",
            "\n",
            "CONTEXT:\n",
            "NOTE_ID: 18427803-DS-5...\n",
            "   Cleaned Answer: Migraine with aura (MWA)\n",
            "\n",
            "SEVERITY: Mild or Moderate...\n",
            "   Accuracy: 1\n",
            "   Grounded: 1\n",
            "   Coherent: True\n",
            "   Relevant Sources: True\n",
            "\n",
            "3. Query: What are the symptoms of migraine with aura?\n",
            "Using semantic search instead.\n",
            "   Expected: ['aura', 'headache', 'visual', 'neurological']\n",
            "   Raw Answer (first 100 chars): You are a clinical assistant. Use ONLY the context below to answer.\n",
            "\n",
            "CONTEXT:\n",
            "• Migraine With Aura\n",
            " ...\n",
            "   Cleaned Answer: There are several common symptoms of migraine with aura:\n",
            "\n",
            "1....\n",
            "   Accuracy: 1\n",
            "   Grounded: 0\n",
            "   Coherent: True\n",
            "   Relevant Sources: True\n",
            "\n",
            "======================================================================\n",
            "FIXED RESULTS SUMMARY\n",
            "======================================================================\n",
            "\n",
            "Retrieval:\n",
            "   Precision:        46.67%\n",
            "   Recall:           100.00%\n",
            "   MRR:              100.00%\n",
            "   Patient Accuracy: 100.00%\n",
            "\n",
            "Generation:\n",
            "   Accuracy:           100.00%\n",
            "   Hallucination Rate: 33.33%\n",
            "   Coherence:          100.00%\n",
            "   Citation Quality:   100.00%\n",
            "\n",
            "======================================================================\n",
            "EVALUATION COMPLETE - FIXED VERSION\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio\n"
      ],
      "metadata": {
        "id": "avO8XBga9Qv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! It sounds like you're looking for a clearer picture of how all these components work together to perform RAG (Retrieval-Augmented Generation). Let me break it down for you:\n",
        "\n",
        "Understanding RAG: Retrieval-Augmented Generation\n",
        "RAG is a powerful technique that combines the strengths of information retrieval systems with the generative capabilities of large language models (LLMs). It helps LLMs provide more accurate, up-to-date, and attributable answers by grounding their responses in specific, verifiable sources.\n",
        "\n",
        "Here's how the RAG process works in this notebook:\n",
        "\n",
        "Retrieval: When you ask a question, the system first looks up relevant information from a predefined knowledge base. In this case, our knowledge base is composed of clinical notes.\n",
        "\n",
        "How it works: Your question is converted into a numerical representation (an \"embedding\"). This embedding is then used to quickly search through a database of pre-computed embeddings of our clinical note chunks. The goal is to find the chunks that are most semantically similar to your question.\n",
        "Augmentation: The pieces of information (the most relevant clinical note chunks) found during the retrieval step are then added to your original question.\n",
        "\n",
        "How it works: These retrieved chunks become the \"context\" for the language model. Instead of just answering based on its general training data, the LLM is explicitly given the specific clinical notes that might contain the answer.\n",
        "Generation: Finally, the augmented prompt (your question + the retrieved context) is fed to the Large Language Model.\n",
        "\n",
        "How it works: The LLM (TinyLlama in this case) uses this provided context, along with its inherent understanding of language and medical concepts, to formulate a concise and relevant answer. It's instructed to only use the provided context for its answer, and to state if the information isn't available.\n",
        "Key Models and Libraries Explained:\n",
        "TinyLlama/TinyLlama-1.1B-Chat-v1.0 (LLM): This is our Generative Model. It's a smaller, efficient Large Language Model that takes the question and the retrieved context as input and generates the human-readable answer. Think of it as the brain that synthesizes information.\n",
        "\n",
        "sentence-transformers/all-MiniLM-L6-v2 (Embedding Model): This is our Embedding Model. Its job is to convert text (both our clinical note chunks and your questions) into dense numerical vectors. These vectors capture the semantic meaning of the text, allowing us to find similar pieces of information very quickly. If two pieces of text have similar meanings, their embeddings will be numerically close.\n",
        "\n",
        "FAISS (Vector Store): This is our Vector Store (or vector database). After the clinical note chunks are converted into embeddings, FAISS is used to store and efficiently search these embeddings. When a query embedding comes in, FAISS can rapidly find the k (e.g., 3) most similar document embeddings in its index.\n",
        "\n",
        "LangChain: This is an Orchestration Framework. It doesn't perform RAG itself, but it provides a structured way to connect all the different components (LLMs, embedding models, vector stores, prompt templates) into a cohesive \"chain\" or \"pipeline\" that performs the RAG process automatically. It simplifies building complex LLM applications.\n",
        "\n",
        "End-to-End RAG Flow (Data Preprocessing to Output):\n",
        "Data Ingestion & Parsing: Raw clinical notes (JSON files from the unRARred archives) are loaded. We specifically extracted input1 through input6 fields to construct the full_text for each note.\n",
        "\n",
        "Text Splitting (Chunking): Long full_text clinical notes are broken down into smaller, overlapping chunks (using RecursiveCharacterTextSplitter). Each chunk is easier for the embedding model to process and more granular for retrieval. Metadata like note_id and diagnosis are attached to each chunk.\n",
        "\n",
        "Embedding Generation: Each chunk is fed into the all-MiniLM-L6-v2 embedding model, which converts it into a numerical vector (embedding).\n",
        "\n",
        "Vector Store Indexing: These embeddings, along with their original text content and metadata, are stored in a FAISS index. This index is optimized for fast similarity searches.\n",
        "\n",
        "User Query: You type a question into the Gradio chat interface.\n",
        "\n",
        "Query Embedding: Your question is also converted into a numerical embedding using the same all-MiniLM-L6-v2 embedding model.\n",
        "\n",
        "Retrieval: The query's embedding is used to search the FAISS vector store for the k (e.g., 3) most relevant clinical note chunks.\n",
        "\n",
        "Context Augmentation: The retrieved chunks are formatted and inserted into a PromptTemplate as the CONTEXT for the LLM.\n",
        "\n",
        "LLM Generation: The complete prompt (including the question and context) is sent to the TinyLlama LLM. The LLM generates an answer, instructed to use only the provided context.\n",
        "\n",
        "Output: The LLM's generated answer, along with the source documents it used, is displayed in the Gradio chat interface.\n",
        "\n",
        "This entire process ensures that the TinyLlama model provides answers that are not only coherent but also directly supported by the provided clinical data, minimizing hallucinations and improving trustworthiness. The Gradio interface further allows you to dynamically add more notes, which immediately become part of this searchable knowledge base.\n",
        "\n",
        "To make this explanation a permanent part of your notebook, I can add a new markdown cell outlining this process. Would you like me to do that?\n",
        "\n",
        "Add RAG Explanation to Notebook: Add a new markdown cell in the notebook that explains the RAG process, the role of each model and library (LLM, Embeddings, FAISS, LangChain), and the end-to-end data preprocessing to output flow."
      ],
      "metadata": {
        "id": "ZGaYXD84vycp"
      }
    }
  ]
}